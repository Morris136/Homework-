{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morris136/Homework-/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Findings from Temperature and Token Limit Experiments\n",
        "\n",
        "1. How did the output change as you increased the temperature?\n",
        "   - At low temperatures (e.g., 0.0), the output was very deterministic and factual, often resembling textbook definitions.\n",
        "   - At medium temperatures (e.g., 0.5), the response became a bit more natural and conversational.\n",
        "   - At high temperatures (e.g., 1.0), the output was more creative, metaphorical, and occasionally verbose, sometimes even humorous.\n",
        "\n",
        "2. When would you prefer a low temperature versus a high one?\n",
        "   - Use a **low temperature (0.0–0.3)** when you need reliable, accurate information like in educational tools, factual summaries, or scientific explanations.\n",
        "   - Use a **high temperature (0.7–1.0)** when you're looking for creativity—such as story writing, brainstorming ideas, or generating fun responses.\n",
        "\n",
        "3. What happened when the max_output_tokens limit was reached?\n",
        "   - The model’s response was **cut off** once the token limit was reached, sometimes mid-sentence.\n",
        "   - Short limits (like 20 tokens) often yielded incomplete or overly brief answers, while higher limits (like 200) allowed full, detailed responses.\n",
        "\n",
        "4. Describe a practical scenario where you would need to set a specific token limit.\n",
        "   - A chatbot embedded in a mobile app may need to limit output to **50 tokens** to avoid overflowing the UI.\n",
        "   - APIs with **billing based on token count** (like Gemini or OpenAI) benefit from tight limits to reduce cost during high usage.\n",
        "\n",
        "5. How could you combine these two parameters to get a short, creative response versus a long, factual one?\n",
        "   - For a **short, creative** response: set `temperature=1.0` and `max_output_tokens=30`.\n",
        "   - For a **long, factual** answer: set `temperature=0.2` and `max_output_tokens=200`.\n",
        "\n",
        "# Conclusion:\n",
        "Understanding how temperature and token limits influence model output allows for greater control over tone, length, and usefulness of AI responses. Balancing both gives you flexibility for different application needs.\n"
      ],
      "metadata": {
        "id": "JNqlZenfIX3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}