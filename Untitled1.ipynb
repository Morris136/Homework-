{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morris136/Homework-/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load dataset and embeddings\n",
        "df = pd.read_csv('movie_plots.csv')  # Contains 'title' column\n",
        "embeddings = np.load('movie_plots_embeddings.npy')  # shape: (num_items, embedding_dim)\n",
        "\n",
        "def find_top_k_similar(embedding, embeddings_matrix, df, top_k=5):\n",
        "    \"\"\"\n",
        "    Find top_k most similar items to the given embedding.\n",
        "\n",
        "    Args:\n",
        "        embedding (np.array): Single embedding vector, shape (embedding_dim,)\n",
        "        embeddings_matrix (np.array): All item embeddings, shape (num_items, embedding_dim)\n",
        "        df (pd.DataFrame): Dataset with titles or metadata\n",
        "        top_k (int): Number of top similar items to return\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: [(title, similarity_score), ...] sorted descending by similarity\n",
        "    \"\"\"\n",
        "    # Reshape embedding to 2D array for sklearn cosine_similarity\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity between input embedding and all stored embeddings\n",
        "    similarities = cosine_similarity(embedding, embeddings_matrix)[0]  # shape: (num_items,)\n",
        "\n",
        "    # Get indices of top_k highest similarity scores\n",
        "    top_k_idx = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # Collect titles and similarity scores\n",
        "    results = [(df.iloc[i]['title'], similarities[i]) for i in top_k_idx]\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Example usage ---\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Pick an example text to get its embedding and find similar movies\n",
        "example_text = df.iloc[0]['plot']\n",
        "example_embedding = model.encode([example_text])[0]\n",
        "\n",
        "top_similar = find_top_k_similar(example_embedding, embeddings, df, top_k=5)\n",
        "\n",
        "print(f\"Top 5 movies similar to: '{df.iloc[0]['title']}'\\n\")\n",
        "for title, score in top_similar:\n",
        "    print(f\"{title} (similarity: {score:.4f})\")\n"
      ],
      "metadata": {
        "id": "kOBal-eVPOO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}